{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b600c-9f73-488e-ad75-063df233260b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import s3fs  # Para interactuar con S3\n",
    "import os  # Para manejar nombres de archivo/ruta\n",
    "import tempfile\n",
    "# No se necesita matplotlib.pyplot en este script, se puede eliminar si se desea.\n",
    "\n",
    "def preprocess_and_save_s3(s3, input_s3_path, output_s3_path, selected_video_ids, target_resolution=(120, 160)):\n",
    "    \"\"\"\n",
    "    Lee datos de video desde un archivo HDF5 en S3, los preprocesa\n",
    "    (ajuste de longitud, redimensionado) y guarda el resultado en\n",
    "    otro archivo HDF5 en S3.\n",
    "\n",
    "    Args:\n",
    "        s3 (s3fs.S3FileSystem): Instancia del sistema de archivos S3.\n",
    "        input_s3_path (str): Ruta S3 completa al archivo HDF5 de entrada.\n",
    "        output_s3_path (str): Ruta S3 completa donde se guardar√° el archivo HDF5 preprocesado.\n",
    "        selected_video_ids (list): Lista de IDs de video a procesar del archivo de entrada.\n",
    "        target_resolution (tuple): Resoluci√≥n (altura, anchura) a la que redimensionar los frames.\n",
    "    \"\"\"\n",
    "    print(f\"Ruta de entrada: {input_s3_path}\")\n",
    "    print(f\"Ruta de salida: {output_s3_path}\")\n",
    "\n",
    "    processed_data = {}  # Diccionario para almacenar datos procesados temporalmente {vid: data}\n",
    "    glosses_map = {}  # Diccionario para almacenar los glosses {vid: gloss}\n",
    "    target_frames = None  # Se determinar√° a partir de los datos\n",
    "\n",
    "    try:\n",
    "        # Abre el archivo HDF5 de entrada directamente desde S3 usando s3fs\n",
    "        print(\"Accediendo al archivo HDF5 de entrada desde S3...\")\n",
    "        with s3.open(input_s3_path, 'rb') as s3_input_file:\n",
    "            with h5py.File(s3_input_file, 'r') as f_in:\n",
    "                print(\"Archivo HDF5 de entrada abierto.\")\n",
    "\n",
    "                # Calcula la longitud mediana de los frames\n",
    "                frame_counts = [f_in[vid]['frames'].shape[0] for vid in selected_video_ids]\n",
    "                print(f\"Se encontraron {len(frame_counts)} secuencias de video.\")\n",
    "                target_frames = int(np.median(frame_counts))\n",
    "                print(f\"Mediana de frames (longitud est√°ndar): {target_frames}\")\n",
    "\n",
    "                # Procesa cada video seleccionado\n",
    "                for i, vid in enumerate(selected_video_ids):\n",
    "                    print(f\"\\nProcesando video {i+1}/{len(selected_video_ids)} (ID: {vid})\")\n",
    "                    video_data = f_in[vid]['frames'][:]  # Carga los datos del video\n",
    "                    gloss = f_in[vid].attrs['gloss']\n",
    "                    glosses_map[vid] = gloss\n",
    "                    \n",
    "                    current_frames = video_data.shape[0]\n",
    "                    \n",
    "                    # --- Ajuste de longitud (Padding o Recorte) ---\n",
    "                    adjusted_video_data = None\n",
    "                    if current_frames < target_frames:\n",
    "                        # print(\"Aplicando padding (interpolaci√≥n)...\")\n",
    "                        indices = np.linspace(0, current_frames - 1, num=target_frames)\n",
    "                        # Vectorizaci√≥n para mayor eficiencia\n",
    "                        idx_lower = np.floor(indices).astype(int)\n",
    "                        idx_upper = np.ceil(indices).astype(int)\n",
    "                        # Asegurarse de que upper no exceda el l√≠mite\n",
    "                        idx_upper[idx_upper >= current_frames] = current_frames - 1\n",
    "                        weight = indices - idx_lower\n",
    "                        \n",
    "                        # Interpola usando los pesos. A√±ade dimensiones para broadcasting\n",
    "                        adjusted_video_data = (1 - weight[:, np.newaxis, np.newaxis, np.newaxis]) * video_data[idx_lower] + \\\n",
    "                                              weight[:, np.newaxis, np.newaxis, np.newaxis] * video_data[idx_upper]\n",
    "                        adjusted_video_data = adjusted_video_data.astype(video_data.dtype)\n",
    "\n",
    "                    elif current_frames > target_frames:\n",
    "                        # print(\"Aplicando recorte (muestreo uniforme)...\")\n",
    "                        indices = np.linspace(0, current_frames - 1, num=target_frames, dtype=int)\n",
    "                        adjusted_video_data = video_data[indices]\n",
    "                    \n",
    "                    else:\n",
    "                        # print(\"La longitud ya es igual a la mediana.\")\n",
    "                        adjusted_video_data = video_data\n",
    "\n",
    "                    # --- Redimensionado de frames ---\n",
    "                    if adjusted_video_data.ndim < 3:\n",
    "                         print(f\"Advertencia: El video {vid} tiene dimensiones inesperadas {adjusted_video_data.shape}. Se omite.\")\n",
    "                         continue\n",
    "                    \n",
    "                    # tf.image.resize espera float, convierte si es necesario\n",
    "                    if adjusted_video_data.dtype != np.float32:\n",
    "                        adjusted_video_data_float = adjusted_video_data.astype(np.float32)\n",
    "                    else:\n",
    "                        adjusted_video_data_float = adjusted_video_data\n",
    "\n",
    "                    resized_video_data = tf.image.resize(adjusted_video_data_float, target_resolution).numpy()\n",
    "\n",
    "                    # --- A√±adir dimensi√≥n de canal si es necesario ---\n",
    "                    if resized_video_data.ndim == 3:\n",
    "                        resized_video_data = resized_video_data[..., np.newaxis]\n",
    "                    \n",
    "                    processed_data[vid] = resized_video_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante la lectura o procesamiento del archivo de entrada: {e}\")\n",
    "        raise\n",
    "\n",
    "    # --- Guardar los datos preprocesados en un nuevo archivo HDF5 ---\n",
    "    if not processed_data:\n",
    "        print(\"No se procesaron datos, no se guardar√° ning√∫n archivo de salida.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nPreparando para guardar {len(processed_data)} videos preprocesados en: {output_s3_path}\")\n",
    "    local_temp_path = None\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".h5\", delete=False) as tmp_h5:\n",
    "            local_temp_path = tmp_h5.name\n",
    "        \n",
    "        with h5py.File(local_temp_path, 'w') as f_out:\n",
    "            print(\"Archivo HDF5 temporal abierto para escritura local.\")\n",
    "            for vid, data in processed_data.items():\n",
    "                grp = f_out.create_group(vid)\n",
    "                grp.create_dataset('frames', data=data, dtype=data.dtype, compression='gzip')\n",
    "                grp.attrs['gloss'] = glosses_map[vid]\n",
    "        \n",
    "        print(\"Subiendo archivo a S3...\")\n",
    "        s3.put(local_temp_path, output_s3_path)\n",
    "        print(\"Archivo subido a S3 exitosamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el guardado local o la subida a S3: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if local_temp_path and os.path.exists(local_temp_path):\n",
    "            print(f\"Eliminando archivo temporal local: {local_temp_path}\")\n",
    "            os.remove(local_temp_path)\n",
    "\n",
    "    print(\"¬°Preprocesamiento del dataset completado!\")\n",
    "    return output_s3_path\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal que orquesta el preprocesamiento de m√∫ltiples datasets.\n",
    "    \"\"\"\n",
    "    # --- Rutas S3 ---\n",
    "    input_s3_folder = \"s3://representatiohorarum/Datasets/Completos/\"\n",
    "    output_s3_folder = \"s3://representatiohorarum/Datasets/Luego del trim/\"\n",
    "\n",
    "    # --- Archivos a procesar (entrada -> salida) ---\n",
    "    datasets = {\n",
    "        \"ISL_glosscomun.h5\": \"ISL_trim.h5\",\n",
    "        \"SLOVO_glosscomun.h5\": \"SLOVO_trim.h5\",\n",
    "        \"WLSL_V03_glosscomun.h5\": \"WLSL_trim.h5\"\n",
    "    }\n",
    "\n",
    "    # Inicializa el sistema de archivos S3\n",
    "    # s3fs usar√° las credenciales de AWS configuradas en el entorno\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "\n",
    "    # Asegurarse de que el directorio de salida exista en S3\n",
    "    if not s3.exists(output_s3_folder):\n",
    "        print(f\"Creando directorio de salida en S3: {output_s3_folder}\")\n",
    "        s3.makedirs(output_s3_folder)\n",
    "\n",
    "    # --- Bucle de procesamiento ---\n",
    "    for input_filename, output_filename in datasets.items():\n",
    "        input_s3_path = os.path.join(input_s3_folder, input_filename)\n",
    "        output_s3_path = os.path.join(output_s3_folder, output_filename)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"INICIANDO PROCESAMIENTO PARA: {input_filename}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            # Obtener todos los IDs de video del archivo de entrada\n",
    "            print(\"Obteniendo IDs de video del archivo de entrada...\")\n",
    "            with s3.open(input_s3_path, 'rb') as temp_s3_file:\n",
    "                with h5py.File(temp_s3_file, 'r') as temp_h5_file:\n",
    "                    ids_to_process = list(temp_h5_file.keys())\n",
    "            \n",
    "            if not ids_to_process:\n",
    "                print(f\"Advertencia: No se encontraron videos en {input_s3_path}. Saltando al siguiente.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Se procesar√°n {len(ids_to_process)} videos.\")\n",
    "            \n",
    "            # Llamar a la funci√≥n principal de preprocesamiento\n",
    "            final_output_path = preprocess_and_save_s3(\n",
    "                s3=s3,\n",
    "                input_s3_path=input_s3_path,\n",
    "                output_s3_path=output_s3_path,\n",
    "                selected_video_ids=ids_to_process,\n",
    "                target_resolution=(120, 160)  # Puedes cambiar la resoluci√≥n aqu√≠\n",
    "            )\n",
    "\n",
    "            if final_output_path:\n",
    "                print(f\"\\nPROCESO EXITOSO para {input_filename}.\")\n",
    "                print(f\"Archivo preprocesado guardado en: {final_output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n{'!'*20} ERROR {'!'*20}\")\n",
    "            print(f\"Ocurri√≥ un error fatal procesando el archivo {input_s3_path}: {e}\")\n",
    "            print(\"Saltando al siguiente dataset si hay m√°s.\")\n",
    "            print(f\"{'!'*47}\")\n",
    "            continue # Contin√∫a con el siguiente archivo en caso de error\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TODOS LOS PROCESOS HAN FINALIZADO.\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# --- Punto de entrada del script ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b3168-d704-4bdf-aeb3-990519f6d6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import s3fs\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def inspect_h5_files_and_show_frames_from_s3():\n",
    "    \"\"\"\n",
    "    Reads H5 files from S3, displays frames of the first 4 video sequences per gloss in each file,\n",
    "    and prints summary information for each sequence.\n",
    "    \"\"\"\n",
    "    # --- S3 Configuration ---\n",
    "    s3_folder = \"s3://representatiohorarum/Datasets/Luego del trim/\"\n",
    "    files_to_inspect = [\n",
    "        \"ISL_trim.h5\",\n",
    "        \"SLOVO_trim.h5\",\n",
    "        \"WLSL_trim.h5\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Inspecting files in: {s3_folder}\\n\")\n",
    "\n",
    "    try:\n",
    "        s3 = s3fs.S3FileSystem()\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing S3FileSystem. Ensure AWS credentials are configured. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    header = (\n",
    "        f\"{'Archivo Origen':<25} | \"\n",
    "        f\"{'ID de Secuencia':<15} | \"\n",
    "        f\"{'Etiqueta (Gloss)':<20} | \"\n",
    "        f\"{'Dimensiones (F, H, W, C)':<28} | \"\n",
    "        f\"{'Tipo de Dato'}\"\n",
    "    )\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for filename in files_to_inspect:\n",
    "        full_s3_path = os.path.join(s3_folder, filename)\n",
    "        gloss_count = {}  # Dictionary to track number of videos per gloss\n",
    "\n",
    "        try:\n",
    "            with s3.open(full_s3_path, 'rb') as s3_file:\n",
    "                with h5py.File(s3_file, 'r') as hf:\n",
    "                    sequence_ids = list(hf.keys())\n",
    "                    if not sequence_ids:\n",
    "                        print(f\"\\nWarning: File {filename} is empty or contains no sequences.\\n\")\n",
    "                        continue\n",
    "\n",
    "                    for seq_id in sequence_ids:\n",
    "                        sequence_group = hf.get(seq_id)\n",
    "                        if sequence_group is None:\n",
    "                            print(f\"Warning: Sequence ID '{seq_id}' not found in {filename}.\")\n",
    "                            continue\n",
    "\n",
    "                        label = sequence_group.attrs.get('gloss', 'N/A')\n",
    "\n",
    "                        # Initialize or update gloss count\n",
    "                        if label not in gloss_count:\n",
    "                            gloss_count[label] = 0\n",
    "                        gloss_count[label] += 1\n",
    "\n",
    "                        # Process only if we haven't exceeded 4 videos for this gloss\n",
    "                        if gloss_count[label] <= 4:\n",
    "                            if 'frames' in sequence_group:\n",
    "                                frames_dataset = sequence_group['frames']\n",
    "                                shape = frames_dataset.shape\n",
    "                                dtype = frames_dataset.dtype\n",
    "\n",
    "                                row = (\n",
    "                                    f\"{filename:<25} | \"\n",
    "                                    f\"{seq_id:<15} | \"\n",
    "                                    f\"{label:<20} | \"\n",
    "                                    f\"{str(shape):<28} | \"\n",
    "                                    f\"{str(dtype)}\"\n",
    "                                )\n",
    "                                print(row)\n",
    "\n",
    "                                # Display all frames in a single row\n",
    "                                num_frames = shape[0]\n",
    "                                fig, axes = plt.subplots(1, num_frames, figsize=(num_frames * 4, 4))\n",
    "                                if num_frames == 1:\n",
    "                                    axes = [axes]  # Ensure axes is iterable for a single frame\n",
    "                                for i, frame in enumerate(frames_dataset):\n",
    "                                    if frame.ndim == 3 and frame.shape[-1] == 1:\n",
    "                                        axes[i].imshow(frame[:, :, 0], cmap='gray')\n",
    "                                    elif frame.ndim == 3:\n",
    "                                        axes[i].imshow(frame)\n",
    "                                    elif frame.ndim == 2:\n",
    "                                        axes[i].imshow(frame, cmap='gray')\n",
    "                                    else:\n",
    "                                        print(f\"Warning: Frame {i} in sequence {seq_id} has unexpected dimensions: {frame.shape}\")\n",
    "                                    axes[i].set_title(f\"Frame {i}\")\n",
    "                                    axes[i].axis('off')  # Turn off axis numbers and ticks\n",
    "                                plt.suptitle(f\"{filename} - {seq_id} - Gloss: {label}\")\n",
    "                                plt.tight_layout()\n",
    "                                plt.show()\n",
    "\n",
    "                            else:\n",
    "                                print(f\"Warning: 'frames' dataset not found in sequence '{seq_id}' of {filename}.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\nERROR: File not found in S3: {full_s3_path}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR: An error occurred processing {filename}: {e}\\n\")\n",
    "\n",
    "    print(\"\\nFinished inspecting all files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inspect_h5_files_and_show_frames_from_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d58ad-0832-4d2b-9ee4-61a48924aa83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646155c6-c918-4005-97d0-f0a5cb6ba715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports necesarios (aseg√∫rate de tenerlos en tu script) ---\n",
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "import s3fs\n",
    "import os\n",
    "import io # <--- IMPORTANTE: Necesario para la soluci√≥n de guardado\n",
    "from ultralytics import YOLO\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Para mostrar una barra de progreso\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "TARGET_SIZE = 224\n",
    "\n",
    "# --- Cargar el modelo YOLO DE SEGMENTACI√ìN ---\n",
    "try:\n",
    "    model = YOLO('yolov8m-seg.pt') \n",
    "    print(\"Modelo YOLO 'yolov8m-seg.pt' cargado exitosamente. ‚úÖ\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo cargar el modelo YOLO. Desc√°rgalo si es necesario. Error: {e}\")\n",
    "    model = None\n",
    "\n",
    "# --- M√âTODOS DE PROCESAMIENTO (CORREGIDOS) ---\n",
    "\n",
    "def process_frame_with_margin(frame, confidence=0.4, margin=15): # CAMBIO: 'confidence_threshold' a 'confidence'\n",
    "    results = model(frame, verbose=False)\n",
    "    if not results or results[0].masks is None or len(results[0].masks) == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    person_indices = np.where(results[0].boxes.cls == 0)[0]\n",
    "    if len(person_indices) == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    boxes = results[0].boxes.xywh[person_indices]\n",
    "    areas = boxes[:, 2] * boxes[:, 3]\n",
    "    best_detection_idx = person_indices[areas.argmax()]\n",
    "    # CAMBIO: 'confidence_threshold' a 'confidence'\n",
    "    if results[0].boxes.conf[best_detection_idx] < confidence:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    mask = results[0].masks[best_detection_idx].data[0].cpu().numpy().astype(np.uint8)\n",
    "    mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    segmented_person = cv2.bitwise_and(frame, frame, mask=mask_resized)\n",
    "    x1, y1, x2, y2 = results[0].boxes.xyxy[best_detection_idx].cpu().numpy().astype(int)\n",
    "    x1, y1 = max(0, x1 - margin), max(0, y1 - margin)\n",
    "    x2, y2 = min(frame.shape[1], x2 + margin), min(frame.shape[0], y2 + margin)\n",
    "    cropped_segmented = segmented_person[y1:y2, x1:x2]\n",
    "    if cropped_segmented.size == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    return cv2.resize(cropped_segmented, (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_frame_with_dilation(frame, confidence=0.4, kernel_size=7): # CAMBIO: 'confidence_threshold' a 'confidence'\n",
    "    results = model(frame, verbose=False)\n",
    "    if not results or results[0].masks is None or len(results[0].masks) == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    person_indices = np.where(results[0].boxes.cls == 0)[0]\n",
    "    if len(person_indices) == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    boxes = results[0].boxes.xywh[person_indices]\n",
    "    areas = boxes[:, 2] * boxes[:, 3]\n",
    "    best_detection_idx = person_indices[areas.argmax()]\n",
    "    # CAMBIO: 'confidence_threshold' a 'confidence'\n",
    "    if results[0].boxes.conf[best_detection_idx] < confidence:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    mask = results[0].masks[best_detection_idx].data[0].cpu().numpy().astype(np.uint8)\n",
    "    mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask_resized, kernel, iterations=1)\n",
    "    segmented_person = cv2.bitwise_and(frame, frame, mask=dilated_mask)\n",
    "    contours, _ = cv2.findContours(dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    main_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(main_contour)\n",
    "    cropped_segmented = segmented_person[y:y+h, x:x+w]\n",
    "    if cropped_segmented.size == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    return cv2.resize(cropped_segmented, (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_frame_with_hybrid(frame, confidence=0.4, kernel_size=7, margin=5): # CAMBIO: 'confidence_threshold' a 'confidence'\n",
    "    results = model(frame, verbose=False)\n",
    "    if not results or results[0].masks is None or len(results[0].masks) == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    person_indices = np.where(results[0].boxes.cls == 0)[0]\n",
    "    if len(person_indices) == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    boxes = results[0].boxes.xywh[person_indices]\n",
    "    areas = boxes[:, 2] * boxes[:, 3]\n",
    "    best_detection_idx = person_indices[areas.argmax()]\n",
    "    # CAMBIO: 'confidence_threshold' a 'confidence'\n",
    "    if results[0].boxes.conf[best_detection_idx] < confidence:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    mask = results[0].masks[best_detection_idx].data[0].cpu().numpy().astype(np.uint8)\n",
    "    mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask_resized, kernel, iterations=1)\n",
    "    segmented_person = cv2.bitwise_and(frame, frame, mask=dilated_mask)\n",
    "    contours, _ = cv2.findContours(dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    main_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(main_contour)\n",
    "    x1, y1 = max(0, x - margin), max(0, y - margin)\n",
    "    x2, y2 = min(frame.shape[1], x + w + margin), min(frame.shape[0], y + h + margin)\n",
    "    cropped_segmented = segmented_person[y1:y2, x1:x2]\n",
    "    if cropped_segmented.size == 0:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "    return cv2.resize(cropped_segmented, (TARGET_SIZE, TARGET_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "# --- FUNCI√ìN \"ROUTER\" (Sin cambios) ---\n",
    "def process_frame_by_method(frame, params):\n",
    "    method = params.get('method', 'margin')\n",
    "    if method == 'margin':\n",
    "        return process_frame_with_margin(frame, **{k: v for k, v in params.items() if k != 'method'})\n",
    "    elif method == 'dilation':\n",
    "        return process_frame_with_dilation(frame, **{k: v for k, v in params.items() if k != 'method'})\n",
    "    elif method == 'hybrid':\n",
    "        return process_frame_with_hybrid(frame, **{k: v for k, v in params.items() if k != 'method'})\n",
    "    else:\n",
    "        return cv2.resize(frame, (TARGET_SIZE, TARGET_SIZE))\n",
    "\n",
    "# --- Funci√≥n de carga de datos (Sin cambios) ---\n",
    "def load_data_from_s3_h5(s3_path):\n",
    "    print(f\"Cargando datos originales desde {s3_path}...\")\n",
    "    original_sequences, gloss_labels, video_ids_list = [], [], []\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    with s3.open(s3_path, 'rb') as h5_file_obj:\n",
    "        with h5py.File(h5_file_obj, 'r') as hf:\n",
    "            video_ids = list(hf.keys())\n",
    "            for video_id in video_ids:\n",
    "                group = hf[video_id]\n",
    "                if 'frames' not in group or 'gloss' not in group.attrs: continue\n",
    "                frames_data = group['frames'][:]\n",
    "                actual_gloss_label = group.attrs.get('gloss', 'N/A')\n",
    "                if actual_gloss_label == 'N/A': continue\n",
    "                original_frames = [cv2.cvtColor(frame.astype(np.uint8), cv2.COLOR_GRAY2BGR) for frame in frames_data]\n",
    "                original_sequences.append(np.array(original_frames))\n",
    "                gloss_labels.append(actual_gloss_label)\n",
    "                video_ids_list.append(video_id)\n",
    "    return original_sequences, gloss_labels, video_ids_list\n",
    "\n",
    "# --- ‚ú® CORREGIDO: Funci√≥n para guardar los datos procesados en S3 ---\n",
    "def save_data_to_s3_h5(s3_path, processed_sequences, original_labels, video_ids):\n",
    "    print(f\"\\nGuardando {len(processed_sequences)} videos procesados en {s3_path}...\")\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    try:\n",
    "        # Usar un b√∫fer en memoria para construir el archivo H5\n",
    "        with io.BytesIO() as buffer:\n",
    "            # Escribir en el b√∫fer como si fuera un archivo\n",
    "            with h5py.File(buffer, 'w') as hf:\n",
    "                for i, video_id in enumerate(tqdm(video_ids, desc=\"Preparando guardado\")):\n",
    "                    group = hf.create_group(video_id)\n",
    "                    frames_to_save = [cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) for frame in processed_sequences[i]]\n",
    "                    frames_to_save_np = np.array(frames_to_save)\n",
    "                    group.create_dataset('frames', data=frames_to_save_np, dtype='uint8', compression=\"gzip\")\n",
    "                    group.attrs['gloss'] = original_labels[i]\n",
    "            \n",
    "            # Una vez que el archivo est√° completo en el b√∫fer, escribirlo de una vez en S3\n",
    "            print(\"Subiendo archivo a S3...\")\n",
    "            with s3.open(s3_path, 'wb') as f:\n",
    "                f.write(buffer.getvalue())\n",
    "\n",
    "        print(f\"Archivo guardado exitosamente en S3. ‚úÖ\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar el archivo en S3: {e}\")\n",
    "\n",
    "# --- üöÄ Flujo de Ejecuci√≥n Principal MODIFICADO ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if model is None:\n",
    "        print(\"El modelo YOLO no est√° cargado. Terminando el script.\")\n",
    "    else:\n",
    "        # --- üí° CONTROL PARA PRUEBAS R√ÅPIDAS ---\n",
    "        # Cambia a False para procesar y guardar el dataset completo.\n",
    "        MODO_PRUEBA = False\n",
    "        NUM_VIDEOS_PRUEBA = 5\n",
    "\n",
    "        # 1. Definir la configuraci√≥n de los datasets a procesar\n",
    "        S3_INPUT_BASE = 's3://representatiohorarum/Datasets/Luego del trim/'\n",
    "        S3_OUTPUT_BASE = 's3://representatiohorarum/Datasets/Luego del fondo/'\n",
    "        \n",
    "        datasets_to_process = [\n",
    "            # Descomenta los que quieras procesar\n",
    "             #{\n",
    "              #   'input_name': 'ISL_trim.h5',\n",
    "               #  'output_name': 'ISL_fondo.h5',\n",
    "                # 'params': {'method': 'hybrid', 'kernel_size': 9, 'margin': 10, 'confidence': 0.4}\n",
    "             #}\n",
    "            # {\n",
    "            #     'input_name': 'WLSL_trim.h5',\n",
    "            #     'output_name': 'WLSL_fondo.h5',\n",
    "            #     'params': {'method': 'hybrid', 'kernel_size': 9, 'margin': 10, 'confidence': 0.4}\n",
    "            # },\n",
    "            {\n",
    "                'input_name': 'SLOVO_trim.h5',\n",
    "                'output_name': 'SLOVO_fondo.h5',\n",
    "                'params': {'method': 'dilation', 'kernel_size': 15, 'confidence': 0.4}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # 2. Bucle principal para procesar cada dataset\n",
    "        for config in datasets_to_process:\n",
    "            input_path = S3_INPUT_BASE + config['input_name']\n",
    "            output_path = S3_OUTPUT_BASE + config['output_name']\n",
    "            processing_params = config['params']\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Iniciando procesamiento para: {config['input_name']}\")\n",
    "            print(f\"Usando m√©todo: {processing_params['method']} con params: {processing_params}\")\n",
    "            if MODO_PRUEBA:\n",
    "                print(f\"‚ö†Ô∏è  MODO DE PRUEBA ACTIVO: Se procesar√°n solo {NUM_VIDEOS_PRUEBA} videos.\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            try:\n",
    "                # Cargar datos\n",
    "                original_sequences, gloss_labels, video_ids = load_data_from_s3_h5(input_path)\n",
    "                \n",
    "                # --- ‚ú® APLICAR MODO DE PRUEBA SI EST√Å ACTIVO ---\n",
    "                if MODO_PRUEBA:\n",
    "                    original_sequences = original_sequences[:NUM_VIDEOS_PRUEBA]\n",
    "                    gloss_labels = gloss_labels[:NUM_VIDEOS_PRUEBA]\n",
    "                    video_ids = video_ids[:NUM_VIDEOS_PRUEBA]\n",
    "\n",
    "                # --- Procesamiento del dataset para guardado ---\n",
    "                print(f\"\\nProcesando {len(original_sequences)} videos del dataset '{config['input_name']}'...\")\n",
    "                all_processed_sequences = []\n",
    "                for seq in tqdm(original_sequences, desc=f\"Procesando {config['input_name']}\"):\n",
    "                    processed_sequence = [process_frame_by_method(frame, processing_params) for frame in seq]\n",
    "                    all_processed_sequences.append(np.array(processed_sequence))\n",
    "\n",
    "                # --- Guardar el resultado en un nuevo archivo H5 en S3 ---\n",
    "                save_data_to_s3_h5(output_path, all_processed_sequences, gloss_labels, video_ids)\n",
    "\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"Error cr√≠tico: No se encontr√≥ el archivo {input_path}. Error: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ocurri√≥ un error inesperado durante el procesamiento de {config['input_name']}: {e}\")\n",
    "                \n",
    "        print(\"\\nTodos los datasets han sido procesados. üéâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f0183-e1dc-4890-a903-ae0d909c3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "# Aseg√∫rate de que esta ruta apunte al archivo que quieres verificar\n",
    "s3_file_path = 's3://representatiohorarum/Datasets/Luego del fondo/ISL_fondo.h5'\n",
    "\n",
    "print(f\"üîé Inspeccionando el archivo: {s3_file_path}\\n\")\n",
    "\n",
    "try:\n",
    "    s3 = s3fs.S3FileSystem()\n",
    "    with s3.open(s3_file_path, 'rb') as f:\n",
    "        with h5py.File(f, 'r') as hf:\n",
    "            \n",
    "            video_ids = list(hf.keys())\n",
    "            num_videos = len(video_ids)\n",
    "            \n",
    "            # 1. Imprimir resumen del contenido\n",
    "            print(\"--- RESUMEN DEL ARCHIVO H5 ---\")\n",
    "            print(f\"Total de videos encontrados: {num_videos}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "            if num_videos == 0:\n",
    "                print(\"El archivo est√° vac√≠o.\")\n",
    "            else:\n",
    "                for video_id in video_ids:\n",
    "                    group = hf[video_id]\n",
    "                    frames = group['frames']\n",
    "                    gloss = group.attrs.get('gloss', 'N/A')\n",
    "                    \n",
    "                    # Imprimir informaci√≥n detallada de cada video\n",
    "                    print(f\"‚ñ∂ Video ID: {video_id}\")\n",
    "                    print(f\"  - Etiqueta (Gloss): '{gloss}'\")\n",
    "                    print(f\"  - N√∫mero de frames: {frames.shape[0]}\")\n",
    "                    print(f\"  - Dimensiones de frames: {frames.shape[1:]} (Alto, Ancho)\")\n",
    "                    print(f\"  - Tipo de datos: {frames.dtype}\")\n",
    "            \n",
    "            # 2. Visualizar todos los frames de todos los videos\n",
    "            if num_videos > 0:\n",
    "                print(\"\\n--- VISUALIZACI√ìN DE TODOS LOS FRAMES ---\")\n",
    "                \n",
    "                for video_id in video_ids:\n",
    "                    frames_data = hf[video_id]['frames'][:]\n",
    "                    gloss = hf[video_id].attrs.get('gloss', 'N/A')\n",
    "                    num_frames = len(frames_data)\n",
    "                    \n",
    "                    # Crear una figura con subplots para todos los frames del video\n",
    "                    # Usar una cuadr√≠cula con un m√°ximo de 5 columnas para mejor legibilidad\n",
    "                    max_cols = 5\n",
    "                    num_rows = (num_frames + max_cols - 1) // max_cols  # Calcular filas necesarias\n",
    "                    fig, axes = plt.subplots(num_rows, min(num_frames, max_cols), \n",
    "                                             figsize=(4 * min(num_frames, max_cols), 4 * num_rows))\n",
    "                    \n",
    "                    # Asegurarse de que axes sea una matriz 2D para facilitar la indexaci√≥n\n",
    "                    if num_frames == 1:\n",
    "                        axes = np.array([[axes]])\n",
    "                    elif num_rows == 1:\n",
    "                        axes = np.array([axes])\n",
    "                    \n",
    "                    # Iterar sobre todos los frames\n",
    "                    for j in range(num_frames):\n",
    "                        row = j // max_cols\n",
    "                        col = j % max_cols\n",
    "                        ax = axes[row, col]\n",
    "                        ax.imshow(frames_data[j], cmap='gray')\n",
    "                        ax.axis('off')\n",
    "                        ax.set_title(f\"{video_id} | '{gloss}'\\nFrame: {j}\")\n",
    "                    \n",
    "                    # Ocultar ejes vac√≠os si el n√∫mero de frames no llena la cuadr√≠cula\n",
    "                    if num_frames < num_rows * max_cols:\n",
    "                        for j in range(num_frames, num_rows * max_cols):\n",
    "                            row = j // max_cols\n",
    "                            col = j % max_cols\n",
    "                            axes[row, col].axis('off')\n",
    "                    \n",
    "                    plt.suptitle(f\"Video: {video_id} | Gloss: {gloss}\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: No se encontr√≥ el archivo en la ruta: {s3_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ocurri√≥ un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8260f423-2273-4766-a295-d0d0c7691af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando el proceso de verificaci√≥n y reparaci√≥n de frames negros ---\n",
      "\n",
      "============================================================\n",
      "‚ñ∂Ô∏è  Procesando archivo: ISL_fondo.h5\n",
      "üíæ Guardando en: ISL_negro.h5\n",
      "============================================================\n",
      "Se encontraron 1355 videos. Verificando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1355/1355 [00:23<00:00, 57.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Verificaci√≥n completada.\n",
      "  - Se repararon 0 frames en 0 videos.\n",
      "‚è´ Subiendo archivo procesado a: s3://representatiohorarum/Datasets/luego de frames negros/ISL_negro.h5\n",
      "   Carga completa.\n",
      "\n",
      "============================================================\n",
      "‚ñ∂Ô∏è  Procesando archivo: SLOVO_fondo.h5\n",
      "üíæ Guardando en: SLOVO_negro.h5\n",
      "============================================================\n",
      "Se encontraron 2120 videos. Verificando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2120/2120 [00:37<00:00, 56.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Verificaci√≥n completada.\n",
      "  - Se repararon 0 frames en 0 videos.\n",
      "‚è´ Subiendo archivo procesado a: s3://representatiohorarum/Datasets/luego de frames negros/SLOVO_negro.h5\n",
      "   Carga completa.\n",
      "\n",
      "\n",
      "--- Proceso de reparaci√≥n de frames completado para todos los archivos. --- üéâ\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "import s3fs\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm # A√±adido para una mejor visualizaci√≥n del progreso\n",
    "\n",
    "def repair_black_frames():\n",
    "    \"\"\"\n",
    "    Busca y reemplaza frames completamente negros en archivos H5 de videos.\n",
    "    Un frame negro se reemplaza por una fusi√≥n del frame no-negro anterior y el siguiente.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- üí° CONFIGURACI√ìN DE RUTAS (SECCI√ìN MODIFICADA) ---\n",
    "    INPUT_BASE_PATH = \"s3://representatiohorarum/Datasets/Luego del fondo/\"\n",
    "    OUTPUT_BASE_PATH = \"s3://representatiohorarum/Datasets/luego de frames negros/\"\n",
    "\n",
    "    # Define los datasets a procesar de forma organizada\n",
    "    datasets_to_process = {\n",
    "        \"ISL\":   {\"input\": \"ISL_fondo.h5\", \"output\": \"ISL_negro.h5\"},\n",
    "        \"SLOVO\": {\"input\": \"SLOVO_fondo.h5\", \"output\": \"SLOVO_negro.h5\"},\n",
    "        #\"WLSL\":  {\"input\": \"WLSL_fondo.h5\", \"output\": \"WLSL_negro.h5\"},\n",
    "    }\n",
    "\n",
    "    # Construye las listas de archivos de entrada y salida din√°micamente\n",
    "    h5_input_files = [INPUT_BASE_PATH + info[\"input\"] for info in datasets_to_process.values()]\n",
    "    h5_output_files = [OUTPUT_BASE_PATH + info[\"output\"] for info in datasets_to_process.values()]\n",
    "    # --- FIN DE LA SECCI√ìN MODIFICADA ---\n",
    "    \n",
    "\n",
    "    print(\"--- Iniciando el proceso de verificaci√≥n y reparaci√≥n de frames negros ---\")\n",
    "\n",
    "    try:\n",
    "        s3 = s3fs.S3FileSystem()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: No se pudo inicializar S3FileSystem. Verifica tus credenciales.\")\n",
    "        print(f\"Detalle del error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Iterar sobre cada par de archivos\n",
    "    for input_path, output_path in zip(h5_input_files, h5_output_files):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚ñ∂Ô∏è  Procesando archivo: {os.path.basename(input_path)}\")\n",
    "        print(f\"üíæ Guardando en: {os.path.basename(output_path)}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        temp_filename = f\"temp_repair_{os.path.basename(output_path)}_{int(time.time())}.h5\"\n",
    "        total_repaired_frames = 0\n",
    "        total_repaired_videos = 0\n",
    "\n",
    "        try:\n",
    "            with s3.open(input_path, 'rb') as s3_file:\n",
    "                with h5py.File(s3_file, 'r') as hf_in:\n",
    "                    with h5py.File(temp_filename, 'w') as hf_out:\n",
    "                        video_ids = list(hf_in.keys())\n",
    "                        print(f\"Se encontraron {len(video_ids)} videos. Verificando...\")\n",
    "\n",
    "                        for video_id in tqdm(video_ids, desc=\"Procesando videos\"):\n",
    "                            group_in = hf_in[video_id]\n",
    "\n",
    "                            if 'frames' not in group_in:\n",
    "                                continue\n",
    "\n",
    "                            frames = group_in['frames'][:]\n",
    "                            \n",
    "                            group_out = hf_out.create_group(video_id)\n",
    "                            for key, value in group_in.attrs.items():\n",
    "                                group_out.attrs[key] = value\n",
    "\n",
    "                            # --- L√≥gica de b√∫squeda y reemplazo (sin cambios) ---\n",
    "                            black_frame_indices = [idx for idx, frame in enumerate(frames) if not np.any(frame)]\n",
    "                            \n",
    "                            if black_frame_indices:\n",
    "                                total_repaired_videos += 1\n",
    "                                total_repaired_frames += len(black_frame_indices)\n",
    "                                \n",
    "                                frames_list = [frame.astype(np.uint8) for frame in frames]\n",
    "\n",
    "                                for idx in black_frame_indices:\n",
    "                                    prev_frame_idx = next((prev_i for prev_i in range(idx - 1, -1, -1) if prev_i not in black_frame_indices), -1)\n",
    "                                    next_frame_idx = next((next_i for next_i in range(idx + 1, len(frames_list)) if next_i not in black_frame_indices), -1)\n",
    "\n",
    "                                    if prev_frame_idx != -1 and next_frame_idx != -1:\n",
    "                                        prev_frame = frames_list[prev_frame_idx]\n",
    "                                        next_frame = frames_list[next_frame_idx]\n",
    "                                        new_frame = cv2.addWeighted(prev_frame, 0.5, next_frame, 0.5, 0)\n",
    "                                        frames_list[idx] = new_frame\n",
    "                                    elif prev_frame_idx != -1:\n",
    "                                        frames_list[idx] = frames_list[prev_frame_idx]\n",
    "                                    elif next_frame_idx != -1:\n",
    "                                        frames_list[idx] = frames_list[next_frame_idx]\n",
    "                                \n",
    "                                frames = np.array(frames_list, dtype=np.uint8)\n",
    "\n",
    "                            # Guardar el dataset de frames (modificado o no)\n",
    "                            group_out.create_dataset('frames', data=frames, compression=\"gzip\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ Verificaci√≥n completada.\")\n",
    "            print(f\"  - Se repararon {total_repaired_frames} frames en {total_repaired_videos} videos.\")\n",
    "            \n",
    "            print(f\"‚è´ Subiendo archivo procesado a: {output_path}\")\n",
    "            s3.put(temp_filename, output_path)\n",
    "            print(\"   Carga completa.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: Ocurri√≥ un error inesperado al procesar '{input_path}': {e}\")\n",
    "        finally:\n",
    "            if os.path.exists(temp_filename):\n",
    "                os.remove(temp_filename)\n",
    "                #print(f\"   Archivo temporal '{temp_filename}' eliminado.\")\n",
    "\n",
    "    print(\"\\n\\n--- Proceso de reparaci√≥n de frames completado para todos los archivos. --- üéâ\")\n",
    "\n",
    "# --- Ejecutar la funci√≥n ---\n",
    "if __name__ == '__main__':\n",
    "    repair_black_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e48ed-822e-4275-acf2-f5ffffe0040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando la verificaci√≥n visual de los datasets procesados ---\n",
      "\n",
      "======================================================================\n",
      "üîé Analizando archivo: SLOVO_negro.h5\n",
      "======================================================================\n",
      "Mostrando todos los 2120 videos...\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "S3_BASE_PATH = \"s3://representatiohorarum/Datasets/luego de frames negros/\"\n",
    "FILENAMES = [#\"ISL_negro.h5\", \n",
    "             \"SLOVO_negro.h5\" \n",
    "             #\"WLSL_negro.h5\"\n",
    "                ]\n",
    "\n",
    "# Puedes ajustar cu√°ntos frames por video quieres ver\n",
    "NUM_FRAMES_PER_VIDEO = 4  # Muestra el primer, medio y √∫ltimo frame\n",
    "\n",
    "print(\"--- Iniciando la verificaci√≥n visual de los datasets procesados ---\")\n",
    "\n",
    "# Inicializa el sistema de archivos de S3\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "# Itera sobre cada archivo final\n",
    "for filename in FILENAMES:\n",
    "    s3_path = S3_BASE_PATH + filename\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîé Analizando archivo: {filename}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    try:\n",
    "        with s3.open(s3_path, 'rb') as s3_file:\n",
    "            with h5py.File(s3_file, 'r') as hf:\n",
    "                all_video_ids = list(hf.keys())\n",
    "\n",
    "                if not all_video_ids:\n",
    "                    print(\"‚ÄºÔ∏è  Este archivo no contiene videos.\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Mostrando todos los {len(all_video_ids)} videos...\")\n",
    "\n",
    "                # Prepara la figura para mostrar los frames de todos los videos\n",
    "                fig, axes = plt.subplots(\n",
    "                    len(all_video_ids), \n",
    "                    NUM_FRAMES_PER_VIDEO, \n",
    "                    figsize=(10, 3.5 * len(all_video_ids))\n",
    "                )\n",
    "                \n",
    "                # Asegura que 'axes' sea siempre 2D para una f√°cil indexaci√≥n\n",
    "                axes = np.atleast_2d(axes)\n",
    "\n",
    "                # Itera sobre todos los videos para mostrarlos\n",
    "                for i, video_id in enumerate(all_video_ids):\n",
    "                    frames = hf[video_id]['frames'][:]\n",
    "                    gloss = hf[video_id].attrs.get('gloss', 'N/A')\n",
    "                    \n",
    "                    # Selecciona los √≠ndices del primer, medio y √∫ltimo frame\n",
    "                    indices_to_plot = sorted(list(set([0, len(frames) // 2, len(frames) - 1])))\n",
    "                    \n",
    "                    for j, frame_idx in enumerate(indices_to_plot):\n",
    "                        ax = axes[i, j]\n",
    "                        ax.imshow(frames[frame_idx], cmap='gray')\n",
    "                        ax.set_title(f\"{video_id}\\n'{gloss}' | Frame {frame_idx}\")\n",
    "                        ax.axis('off')\n",
    "                    \n",
    "                    # Oculta los ejes no utilizados si un video tiene menos de 3 frames\n",
    "                    for j in range(len(indices_to_plot), NUM_FRAMES_PER_VIDEO):\n",
    "                        axes[i, j].axis('off')\n",
    "                        \n",
    "                fig.suptitle(f\"Muestra de Frames para '{filename}'\", fontsize=16, y=1.02)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: No se encontr√≥ el archivo en la ruta: {s3_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ocurri√≥ un error inesperado al procesar {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee937deb-dfa0-4b71-af48-6500ba573911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
