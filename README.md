# ğŸ‘¨â€ğŸ”¬ Scientific Investigation on CSLR and Temporal Sensitivity ğŸ’»

Continuous Sign Language Recognition (CSLR) is a specialized field within computer vision and artificial intelligence that focuses on interpreting and understanding sign language from video sequences. Unlike isolated sign recognition, CSLR deals with continuous signing where multiple signs are connected and form complete phrases or sentences.

---

## ğŸ’½ The core components of our intelligent CSLR system include:

- **Autoencoder (AE)**: Learns compressed representations of sign language video sequences

- **3D Convolutional Networks**: Extract spatial and temporal features from video data

- **Bidirectional GRU**: Processes temporal sequences in both directions for better context understanding

- **Multi-dataset Support**: Handles different sign language datasets with unified preprocessing

- This architecture transforms raw video input into meaningful gloss labels, enabling automatic understanding of continuous sign language communication.

---

## ğŸš€ Features

ğŸ“¹ **Video Sequence Processing**: Handles video input for continuous sign language recognition in a word level

ğŸ¯ **Data Preprocessing**: Clear pipeline for video sequence normalization and labeling

ğŸ§  **Deep Learning Architecture**: Combines 3D CNNs with bidirectional GRU networks

ğŸ”„ **Autoencoder Framework**: Learns efficient representations of sign language patterns through pretext tasks

ğŸ“Š **Multi-dataset Support**: Compatible with WLASL, ISL, and SLOVO datasets

ğŸ·ï¸ **Gloss Analisis**: Creates a visual representation of the behavior of each gloss for video sequences

ğŸ“ˆ **Temporal Evaluation**: Comprehensive metrics and temporal sensitivity analysis

---

## ğŸ§  Technical Architecture

### Core Components

- 3D Convolutional Autoencoder

 - Learns compressed representations of video sequences
 - Processes spatial and temporal dimensions simultaneously
 - Captures motion patterns in sign language gestures

- Bidirectional GRU Network

  - Processes temporal sequences in forward and backward directions
  - Maintains context from both past and future states
  - Enhances classification accuracy for continuous signing

- Multi-variant Processing

  - Generates 5 variants of each video sequence for robustness
  - Data augmentation techniques for improved generalization
  - Help organize the sequence in a latent space using triplet loss

---

## ğŸ“‹ Dataset Support

### WLASL Dataset


- American Sign Language (ASL) video collection
- Structured JSON metadata with gloss labels
- Multiple video files with standardized naming convention


### ISL Dataset

- Indian Sign Language video sequences
- Custom format requiring preprocessing adaptation
- More videos but less glosses


### SLOVO Dataset

- Slovenian Sign Language recordings
- Russian-origin data requiring translation and adaptation
- Special preprocessing for language-specific features

---

## ğŸ”§ Data Preprocessing Pipeline

### 1. Data Restructuring

- Converts various dataset formats into unified H5 structure
- Standardizes video sequences with consistent frame dimensions
- Normalizes temporal sequences to fixed length
  
### 2. Label Translation

- Automatic translation of non-English gloss labels
- Manual verification of translation accuracy
- JSON mapping for language-specific terms

### 3. Sequence Normalization

- Resizes frames to standardized dimensions (120x160)
- Normalizes sequence length
- Handles aspect ratio preservation and quality enhancement

### 4. Background Processing

- Background removal for improved feature extraction using YOLOv8
- Focus on signer silhouette and gesture patterns
- Enhanced contrast and clarity adjustments

---

## ğŸ—ï¸ Implementation Structure

### Model Architecture

Input Video Sequence (N x F x 120 x 160 x 1)
    â†“
3D Convolutional Autoencoder
    â†“
Compressed Feature Representation
    â†“
Bidirectional GRU Processing
    â†“
Temporal Context Integration
    â†“
Gloss Visualization Output

### Training Components

- **Reconstruction Loss**: Ensures accurate video sequence reconstruction
- **Triplet Loss (Inter-gloss)**: Separates different gloss by areas in a latent space
- **Triplet Loss (Temporal)**: Maintains temporal consistency
- **Combined Loss Function**: Weighted combination of all loss terms

---

## ğŸ“Š Performance Metrics

### Training Metrics

- Total Loss (combined reconstruction and classification)
- Reconstruction Loss (video sequence accuracy)
- Inter-gloss Triplet Loss (category separation)
- Temporal Triplet Loss (sequence consistency)

### Evaluation Metrics

- Cosine distance and L2 distance
- Temporal sensitivity analysis
- Cross-dataset performance comparison
- Per-sequence agrupation clusters analysis

---

## ğŸ”¬ Experimental Results

The system has been evaluated across multiple datasets with 2 and 3 glosses:



WLASL Experiments: 2-label to full dataset classification

ISL Dataset Tests: Irish Sign Language recognition performance

SLOVO Evaluations: Slovenian Sign Language processing accuracy

Cross-dataset Analysis: Generalization across different signing styles


Each experiment includes:



Training progression analysis

Temporal sensitivity evaluation

Confusion matrix generation

Performance comparison tables



ğŸ“ˆ Future Enhancements


ğŸ”„ Real-time Processing: Optimized inference for live sign language recognition

ğŸŒ Multi-language Support: Extended dataset compatibility

ğŸ“± Mobile Deployment: Lightweight model variants for mobile devices

ğŸ¯ Fine-tuning Capabilities: Domain-specific adaptation mechanisms

ğŸ“Š Advanced Analytics: Detailed performance breakdown and error analysis



ğŸ“– Usage Examples

Dataset Preparation

python
copy
download
# Load and preprocess datasets
from cslr_preprocessing import DataProcessor

processor = DataProcessor()
processed_data = processor.prepare_dataset('WLASL', num_variants=5)

Model Training

python
copy
download
# Initialize and train the CSLR model
from cslr_model import CSLRAutoencoder

model = CSLRAutoencoder(input_shape=(7, 120, 160, 1))
model.train(processed_data, epochs=100)

Inference

python
copy
download
# Predict gloss labels for new sequences
predictions = model.predict(video_sequence)
gloss_labels = processor.decode_predictions(predictions)

---

## ğŸ‘¨â€ğŸ’» Author

Project created by **Daniel Bernal**
